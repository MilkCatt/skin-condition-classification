{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up saving model path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "folder_to_save = 'trained_models/first_pass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5584\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.48      0.52        56\n",
      "           1       0.39      0.52      0.45        21\n",
      "           2       0.35      0.39      0.37        18\n",
      "           3       0.69      0.69      0.69        59\n",
      "\n",
      "    accuracy                           0.56       154\n",
      "   macro avg       0.50      0.52      0.51       154\n",
      "weighted avg       0.57      0.56      0.56       154\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[27 10  6 13]\n",
      " [ 4 11  4  2]\n",
      " [ 4  4  7  3]\n",
      " [12  3  3 41]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load images and create raw pixel embeddings\n",
    "def load_images_and_labels(image_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {\n",
    "        'Monkeypox': 0,\n",
    "        'Chickenpox': 1,\n",
    "        'Measles': 2,\n",
    "        'Normal': 3\n",
    "    }\n",
    "\n",
    "    for class_name, label in label_map.items():\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, image_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (224, 224))  # Resize to 224x224\n",
    "            img = img.flatten()  # Flatten the image to raw pixel embedding\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load images from the dataset\n",
    "image_dir = 'data'  # Base directory\n",
    "X, y = load_images_and_labels(image_dir)\n",
    "\n",
    "# First, split the dataset into 85% train+validation and 15% test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Next, split the train+validation set into 70% train and 15% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Train the logistic regression model with early stopping based on validation accuracy\n",
    "def train_logistic_regression_with_early_stopping(X_train, y_train, X_val, y_val, max_epochs=100, patience=5):\n",
    "    best_model = None\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f'Epoch {epoch+1}/{max_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Train Logistic Regression model\n",
    "        model = LogisticRegression(max_iter=10000, solver='lbfgs', multi_class='multinomial')\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Validation phase\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, val_preds)\n",
    "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        # Check if this is the best accuracy we've seen\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = model  # Save the best model\n",
    "            epochs_no_improve = 0  # Reset the counter if validation improves\n",
    "            print(f\"Validation accuracy improved to {val_acc:.4f}, saving model...\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement. Epochs without improvement: {epochs_no_improve}\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs of no improvement.\")\n",
    "            break\n",
    "\n",
    "    return best_model, best_acc\n",
    "\n",
    "# Train the logistic regression model with early stopping\n",
    "logistic_regression_best_model, best_val_acc = train_logistic_regression_with_early_stopping(X_train, y_train, X_val, y_val, max_epochs=100, patience=5)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_logistic_regression(model, X_test, y_test):\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    # Classification report and confusion matrix\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, test_preds, target_names=['Monkeypox', 'Chickenpox', 'Measles', 'Normal']))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, test_preds))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "evaluate_logistic_regression(logistic_regression_best_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "import joblib\n",
    "model_file = f'{folder_to_save}/logistic_regression.pkl'\n",
    "\n",
    "joblib.dump(logistic_regression_best_model, model_file)\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5649\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59        56\n",
      "           1       0.35      0.43      0.38        21\n",
      "           2       0.33      0.39      0.36        18\n",
      "           3       0.73      0.64      0.68        59\n",
      "\n",
      "    accuracy                           0.56       154\n",
      "   macro avg       0.50      0.51      0.51       154\n",
      "weighted avg       0.58      0.56      0.57       154\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[33  9  6  8]\n",
      " [ 5  9  4  3]\n",
      " [ 4  4  7  3]\n",
      " [13  4  4 38]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load images and create raw pixel embeddings\n",
    "def load_images_and_labels(image_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {\n",
    "        'Monkeypox': 0,\n",
    "        'Chickenpox': 1,\n",
    "        'Measles': 2,\n",
    "        'Normal': 3\n",
    "    }\n",
    "\n",
    "    for class_name, label in label_map.items():\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, image_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (224, 224))  # Resize to 224x224\n",
    "            img = img.flatten()  # Flatten the image to raw pixel embedding\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load images from the dataset\n",
    "image_dir = 'data'  # Base directory\n",
    "X, y = load_images_and_labels(image_dir)\n",
    "\n",
    "# First, split the dataset into 85% train+validation and 15% test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Next, split the train+validation set into 70% train and 15% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Train the SVM model with early stopping based on validation accuracy\n",
    "def train_svm_with_early_stopping(X_train, y_train, X_val, y_val, max_epochs=100, patience=5):\n",
    "    best_model = None\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f'Epoch {epoch+1}/{max_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Train the SVM model\n",
    "        model = SVC(kernel='linear', C=1, decision_function_shape='ovr')  # Linear kernel\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Validation phase\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, val_preds)\n",
    "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        # Check if this is the best accuracy we've seen\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = model  # Save the best model\n",
    "            epochs_no_improve = 0  # Reset the counter if validation improves\n",
    "            print(f\"Validation accuracy improved to {val_acc:.4f}, saving model...\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement. Epochs without improvement: {epochs_no_improve}\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs of no improvement.\")\n",
    "            break\n",
    "\n",
    "    return best_model, best_acc\n",
    "\n",
    "# Train the SVM model with early stopping\n",
    "best_svm_model, best_val_acc = train_svm_with_early_stopping(X_train, y_train, X_val, y_val, max_epochs=100, patience=5)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_svm(model, X_test, y_test):\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    # Classification report and confusion matrix\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, test_preds, target_names=['Monkeypox', 'Chickenpox', 'Measles', 'Normal']))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, test_preds))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "evaluate_svm(best_svm_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_models/first_pass/svm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "import joblib\n",
    "model_file = f'{folder_to_save}/svm.pkl'\n",
    "\n",
    "joblib.dump(best_svm_model, model_file)\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "import joblib\n",
    "model_file = 'trained_models/first_pass/best_svm_model.pkl'\n",
    "\n",
    "joblib.dump(best_svm_model, model_file)\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Lucas/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Training Loss: 1.0011 Acc: 0.6338\n",
      "Validation Accuracy: 0.4655\n",
      "Validation accuracy improved to 0.4655, saving model weights...\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 159\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, best_acc\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m trained_model, best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[0;32m    162\u001b[0m model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/early_stopping_resnet50.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[22], line 108\u001b[0m, in \u001b[0;36mtrain_model_with_early_stopping\u001b[1;34m(model, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[0;32m    105\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Track loss and accuracy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Load images and create raw pixel embeddings\n",
    "def load_images_and_labels(image_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {\n",
    "        'Monkeypox': 0,\n",
    "        'Chickenpox': 1,\n",
    "        'Measles': 2,\n",
    "        'Normal': 3\n",
    "    }\n",
    "\n",
    "    for class_name, label in label_map.items():\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, image_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (224, 224))  # Resize to 224x224 for ResNet\n",
    "            img = np.transpose(img, (2, 0, 1))  # Convert to CxHxW format\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load images from the dataset\n",
    "image_dir = 'data'  # Base directory\n",
    "X, y = load_images_and_labels(image_dir)\n",
    "\n",
    "# First, split the dataset into 85% train+validation and 15% test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Next, split the train+validation set into 70% train and 15% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define a ResNet model (ResNet50) and modify the final layer for 4-class classification\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Replace the final fully connected layer (ResNet50 has 2048 input features to the final layer)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 4)  # 4 classes (Monkeypox, Chickenpox, Measles, Normal)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to train the model with early stopping based on validation accuracy\n",
    "def train_model_with_early_stopping(model, criterion, optimizer, num_epochs=10, patience=3):\n",
    "    best_model_wts = None\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    stop_training = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss and accuracy\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        # Check if this is the best accuracy we've seen\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = model.state_dict()  # Save the best model weights\n",
    "            epochs_no_improve = 0  # Reset the counter if validation improves\n",
    "            print(f\"Validation accuracy improved to {val_acc:.4f}, saving model weights...\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement. Epochs without improvement: {epochs_no_improve}\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs of no improvement.\")\n",
    "            stop_training = True\n",
    "\n",
    "    # Load best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(\"Loaded best model weights.\")\n",
    "\n",
    "    return model, best_acc\n",
    "\n",
    "# Train the model with early stopping\n",
    "trained_model, best_val_acc = train_model_with_early_stopping(model, criterion, optimizer, num_epochs=10, patience=3)\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, dataloader):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = running_corrects.double() / len(dataloader.dataset)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Classification report and confusion matrix\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=['Monkeypox', 'Chickenpox', 'Measles', 'Normal']))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(trained_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_file = f'{folder_to_save}/early_stopping_resnet50.pth'\n",
    "torch.save(trained_model.state_dict(), model_file)\n",
    "print(f\"Best model saved to {model_file} with validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lucas\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Train the Vision Transformer model\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Function to evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, dataloader):\n",
      "Cell \u001b[1;32mIn[20], line 90\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     87\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Track the loss and accuracy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\Documents\\GitHub\\skin-condition-classification\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import timm  # For loading the pre-trained ViT model\n",
    "\n",
    "# Load images and create raw pixel embeddings\n",
    "def load_images_and_labels(image_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {\n",
    "        'Monkeypox': 0,\n",
    "        'Chickenpox': 1,\n",
    "        'Measles': 2,\n",
    "        'Normal': 3\n",
    "    }\n",
    "\n",
    "    for class_name, label in label_map.items():\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, image_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (224, 224))  # Resize to 224x224 for ViT\n",
    "            img = np.transpose(img, (2, 0, 1))  # Convert to CxHxW format (ViT expects this)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load images from the dataset\n",
    "image_dir = 'data'  # Base directory\n",
    "X, y = load_images_and_labels(image_dir)\n",
    "\n",
    "# First, split the dataset into 85% train+validation and 15% test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Next, split the train+validation set into 70% train and 15% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load a pre-trained Vision Transformer (ViT) model from the timm library\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=4)  # 4 classes\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to train the model with early stopping based on validation accuracy\n",
    "def train_model_with_early_stopping(model, criterion, optimizer, num_epochs=10, patience=3):\n",
    "    best_model_wts = None\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    stop_training = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss and accuracy\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        # Check if this is the best accuracy we've seen\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = model.state_dict()  # Save the best model weights\n",
    "            epochs_no_improve = 0  # Reset the counter if validation improves\n",
    "            print(f\"Validation accuracy improved to {val_acc:.4f}, saving model weights...\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement. Epochs without improvement: {epochs_no_improve}\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs of no improvement.\")\n",
    "            stop_training = True\n",
    "\n",
    "    # Load best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(\"Loaded best model weights.\")\n",
    "\n",
    "    return model, best_acc\n",
    "\n",
    "# Train the model with early stopping\n",
    "trained_model, best_val_acc = train_model_with_early_stopping(model, criterion, optimizer, num_epochs=10, patience=3)\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = running_corrects.double() / len(dataloader.dataset)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Classification report and confusion matrix\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=['Monkeypox', 'Chickenpox', 'Measles', 'Normal']))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(trained_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_file = f'{folder_to_save}/early_stopping_vit.pth'\n",
    "torch.save(trained_model.state_dict(), model_file)\n",
    "print(f\"Best model saved to {model_file} with validation accuracy: {best_val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
